{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import scipy as scp\n", "import sklearn as sk\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn import model_selection as ms\n", "import matplotlib.pyplot as plt\n", "from sklearn import metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cellA = pd.read_csv(\"celltypeA.txt\",index_col=0, header=0)\n", "cellB = pd.read_csv(\"celltypeB.txt\", index_col=0, header=0)\n", "cellC = pd.read_csv(\"celltypeC.txt\", index_col=0, header=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["generate random data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data1 = cellA\n", "data2 = cellB"]}, {"cell_type": "markdown", "metadata": {}, "source": ["combine the datasets and then create the labels for dataset 1 as positive and dataset 2 as negative<br>\n", "you can reverse the positive and negative labels by switching the ones and zeros associated with each set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X1 = np.concatenate((data1, data2))\n", "Y1 = np.concatenate((np.ones(len(data1)), np.zeros(len(data2)))) #data1 as positive(1) and data2 as negative(0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["store the values or auroc and aupr to make calculation on"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["aurocToAverage = []\n", "auprToAverage = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["score which will be later used to make calculation for aupr and auroc"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["score = np.zeros(len(Y1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["this function will divide out model into training and testing data, 5 splits"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stratifiedKFold = sk.model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["this function will create our logistic regression to train on"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["linModel = LogisticRegression(class_weight='balanced', random_state=0, max_iter=1000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["we will split our dataset into the splits here, the split function will break our dataset into<br>\n", "the correct sizes and allow us to create 5 different training and test datasets to average over"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for train_index, test_index in stratifiedKFold.split(X1, Y1):\n", "    testX1 = X1[test_index]\n", "    trainX1 = X1[train_index]\n", "    trainY1 = Y1[train_index]\n", "    testY1 = Y1[test_index]\n\n", "    # train the linear model on our training data\n", "    linModel.fit(trainX1, trainY1)\n\n", "    # predict the correct labels for our test dataset\n", "    predY = linModel.predict_proba(testX1)[:, 1]\n", "    score[test_index] = predY.copy()\n\n", "    # calculate the auroc and aupr and then add them to our lists to average\n", "    aurocToAverage.append(metrics.roc_auc_score(testY1, predY))\n", "    auprToAverage.append(metrics.average_precision_score(testY1, predY))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"For cellA positive and cellB negative samples:\")\n", "print(\"AUROC:\" + str((sum(aurocToAverage) / len(aurocToAverage))))\n", "print(\"AUPR:\" + str((sum(auprToAverage) / len(auprToAverage))))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["alculates the false positive rate and true positive rate from our known values and our predicted scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lr_fpr, lr_tpr, _ = sk.metrics.roc_curve(Y1, score)\n", "x = np.linspace(0, 10, 1000)\n", "plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n", "plt.plot(range(1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["alculates the precision and recall from our known values and predicted scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lr_precision, lr_recall, _ = sk.metrics.precision_recall_curve(Y1, score)\n", "plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n", "plt.plot(range(1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.title(\"ROC Precision-Recall and Curve Plot\")\n", "plt.xlabel(\"FPR/Recall\")\n", "plt.ylabel(\"TPR/Precision\")\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}